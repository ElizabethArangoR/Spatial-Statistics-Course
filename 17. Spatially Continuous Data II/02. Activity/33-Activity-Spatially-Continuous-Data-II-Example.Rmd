---
title: "16 Session 16: Spatially Continuous Data II"
output: html_notebook
---

#Practice questions

Answer the following questions:

1. What is a confidence interval?
2. How does a confidence interval vary with the level of significance?
3. Residuals of trend surface analysis are always spatially independent, true or false.
4. Estimates of the prediction error $\hat{\epsilon}_p$ can be obtained from trend surface analysis, true or false. Explain.
5. In your own words describe the concepts of accuracy and precision in spatial interpolation.

#Learning objectives

In this activity, you will:

1. Use trend surface analysis to interpolate a field.
2. Calculate the degree of uncertainty.
3. Think about the role of residual autocorrelation in interpolation.

#Suggested reading

O'Sullivan D and Unwin D (2010) Geographic Information Analysis, 2nd Edition, Chapters 8 and 9. John Wiley & Sons: New Jersey.

#Preliminaries

For this activity you will need the following:

* This R markdown notebook.

* A file called "Wolfcamp Aquifer.RData"

The data is a set of piezometric head (watertable pressure) observations of the Wolfcamp Aquifer in Texas (https://en.wikipedia.org/wiki/Hydraulic_head). Measures of pressure can be used to infer the flow of underground water, since water flows from high to low pressure areas.

These data were collected to evaluate potential flow of contamination related to a high level toxic waste repository in Texas. The Deaf Smith county site in Texas was one of three potential sites proposed for this repository. Beneath the site is a deep brine aquifer known as the Wolfcamp aquifer that may serve as a conduit of contamination leaking from the repository.

The data set consists of 85 georeferenced measurements of piezometric head. Possible applications of interpolation are to determine sites at risk and to quantify uncertainty of the interpolated surface, to evaluate the best locations for monitoring stations.

It is good practice to clear the working space to make sure that you do not have extraneous items there when you begin your work. The command in R to clear the workspace is `rm` (for "remove"), followed by a list of items to be removed. To clear the workspace from _all_ objects, do the following:
```{r}
rm(list = ls())
```

Note that `ls()` lists all objects currently on the worspace.

Load the libraries you will use in this activity (load other packages as appropriate). 
```{r}
library(tidyverse)
library(plotly)
library(spdep)
```

Load dataset:
```{r}
load("Wolfcamp Aquifer.RData")
```

#Activity

1. Estimate trend surface for the dataset. Select a polynomial that provides the best fit (hint: consider the coefficient of multiple determination R^2 and the standard error, in addition to the significance of the parameters). Justify your decision.

Add polynomial terms:
```{r}
aquifer <- mutate(aquifer, X3 = X^3, X2Y = X^2 * Y, X2 = X^2,
                  XY = X * Y,
                  Y2 = Y^2, XY2 = X * Y^2, Y3 = Y^3)
```

Linear:
```{r}
trend1 <- lm(formula = H ~ X + Y, data = aquifer)
summary(trend1)
```

Quadratic:
```{r}
trend2 <- lm(formula = H ~ X^2 + X + XY + Y + Y^2, data = aquifer)
summary(trend2)
```

Cubic:
```{r}
trend3 <- lm(formula = H ~ X^3 + X2Y + X2 + X + XY + Y + Y2 + XY2 + Y3, data = aquifer)
summary(trend3)
```

Based on this, the most likely choice is the cubic trend model.

2. Create an interpolation grid, and use the function `predict` to interpolate the field using your chosen model. Plot the interpolated field using a method of your choice (e.g., `ggplot2`, 3D plot, etc.)

Find the domain of the coordinates:
```{r}
summary(aquifer[,1:2])
```

Coordinates for prediction:
```{r}
X.p <- seq(-140, to = 110, by = 5)
Y.p <- seq(10, to = 180, by = 5)
```

Use these to create interpolation grid:
```{r}
df.p <- expand.grid(X = X.p, Y = Y.p)
```

Calculate polynomial terms:
```{r}
df.p <- mutate(df.p, X3 = X^3, X2Y = X^2 * Y, X2 = X^2,
                  XY = X * Y,
                  Y2 = Y^2, XY2 = X * Y^2, Y3 = Y^3)
```

Obtain predictions:
```{r}
preds <- predict(trend3, newdata = df.p, se.fit = TRUE, interval = "prediction", level = 0.95)
```

Append the predictions, intervals, standard errors to the prediction dataframe:
```{r}
df.p <- data.frame(df.p, z.p = preds$fit[,1], lwr = preds$fit[,2], upr = preds$fit[,3], se.fit = preds$se.fit)
```

Plot the trend surface:
```{r}
ggplot(data = aquifer, aes(x = X, y = Y)) +
  geom_tile(data = df.p, aes(x = X, y= Y, fill = z.p)) +
  scale_fill_distiller(palette = "YlOrRd", trans = "reverse") +
  geom_point(aes(color = H)) +
  scale_color_distiller(palette = "YlOrRd", trans = "reverse") +
  geom_point(shape = 1, size = 2) +
  coord_equal()
```

As an alternative:

Convert predictions to matrix for plotting in `plotly`:
```{r}
z.p <- matrix(preds$fit[,1], nrow = 35, ncol = 51, byrow = TRUE)
```

Plot:
```{r}
plot_ly(x = ~X.p, y = ~Y.p, z = ~z.p, type = "surface") %>% 
  add_markers(data = aquifer, x = ~X, y = ~Y, z = ~H)
```

3. Inspect the confidence intervals (these are an output of `predict`).

The simplest way to do this is by looking at the descriptive statistics of the fit:
```{r}
summary(preds$fit)
```

The range can be plotted as follows. First append the fit and confidence interval to dataframe:
```{r}
df.p <- data.frame(df.p, z.p = preds$fit[,1], lwr = preds$fit[,2], upr = preds$fit[,3])
```

Then plot:
```{r}
ggplot(data = df.p, aes(x = X, y = Y, fill = lwr)) +
  geom_tile() +
  scale_fill_distiller(palette = "YlOrRd", trans = "reverse") +
  coord_equal()

ggplot(data = df.p, aes(x = X, y = Y, fill = upr)) +
  geom_tile() +
  scale_fill_distiller(palette = "YlOrRd", trans = "reverse") +
  coord_equal()
```

Or in `plotly`:
```{r}
z.p_l <- matrix(data = preds$fit[,2], nrow = 35, ncol = 51, byrow = TRUE)
z.p_u <- matrix(data = preds$fit[,3], nrow = 35, ncol = 51, byrow = TRUE)
```

Plot:
```{r}
plot_ly(x = ~X.p, y = ~Y.p, z = ~z.p, type = "surface", colors = "YlOrRd") %>% 
  add_surface(x = ~X.p, y = ~Y.p, z = ~z.p_l, opacity = 0.5, showscale = FALSE) %>%
  add_surface(x = ~X.p, y = ~Y.p, z = ~z.p_u, opacity = 0.5, showscale = FALSE) %>%
  add_markers(data = aquifer, x = ~X, y = ~Y, z = ~H)
```

4. Inspect the residuals of the model. Are they spatially random? If not, what would be the implications for spatial interpolation?

Label the residuals:
```{r}
aquifer$residuals <- ifelse(trend3$residuals > 0, "positive", "negative")
```

Plot:
```{r}
ggplot(data = aquifer, aes(x = X, y = Y, color = residuals)) +
  geom_point() + 
  coord_equal()
```

The plot suggests pockets of positive and negative residuals. Conduct test with Moran's I.

First create spatial weights:
```{r}
w <- knearneigh(as.matrix(aquifer[,1:2]), k = 5) %>% knn2nb() %>% nb2listw()
```

Moran's test:
```{r}
moran.test(trend3$residuals, w)
```

The test fails to reject the null hypothesis, which means that there is residual pattern. If you were asked to guess the value of the residual at location [-25,100], would you say it was most likely to be positive or negative? How can we use this information to improve the quality of our predictions?
